# Data Dictionary Generator

A Python package to automatically generate data dictionaries for clinical datasets using a large language model (LLM) via Ollama. This tool takes in dataset files (CSV format), processes them, and generates descriptions for each column in the dataset, as well as other metadata like data types, sample data, and table descriptions.

## Features
- Automatically generate descriptions for each column in a dataset.
- Handle multiple CSV files in a folder.
- Generate table-level descriptions along with column descriptions.
- Retrieve data type information using sample data for each column.
- Support integration with Ollama LLM for generating metadata descriptions.
- Option to save the generated metadata in CSV format.
- Generates a `data_quality.csv` file containing a summary of the data quality for each column.

## Requirements

Make sure you have Python 3.8+ installed, along with the following dependencies:

- **pandas**: For handling and processing the dataset.
- **requests**: For HTTP requests (if needed).
- **ollama**: For generating metadata descriptions using Ollama's LLM.
- **torch**: PyTorch for deep learning operations (used with Ollama).
- **transformers**: Hugging Face Transformers library, if you're using other LLMs.

Install dependencies using the following command:
```bash
pip install -r requirements.txt
```

## Installation

Clone the repository:

```bash
git clone https://github.com/rafgia/data-dictionary-generator.git
cd data-dictionary-generator
```

Install the required Python packages:

```bash
pip install -r requirements.txt
```

## Usage

### Command-line Interface

Once the package is installed, you can use the command line to generate metadata for your dataset(s).

To run the tool, use the following command:

```bash
python cli.py <folder_path> <dataset_name> <output_file> --model <ollama_model> --format <format>
```

#### Parameters:
- `<folder_path>`: The path to the folder containing your CSV files.
- `<dataset_name>`: The name of your dataset (e.g., `MIMIC`).
- `<output_file>`: The name of the output CSV file where the metadata will be saved.
- `--model`: (Optional) Specify the Ollama model to use for generating metadata (default is `deepseek-r1:1.5b`).
- `--format`: where <format> can be one of csv, json, pdf, markdown (default is csv). 

### Example

1. **Prepare your data files**:
   Place all your CSV files (representing tables in your dataset) in a folder, e.g., `data/MIMIC`.

2. **Run the generator**:

   ```bash
   python cli.py data/MIMIC MIMIC metadata_output.csv --model deepseek-r1:1.5b --format csv
   ```

This will generate metadata for each column in the dataset and save it to a CSV file (`metadata_output.csv`).

### Sample Output

For each **table** in your dataset, the following metadata will be generated:
- **Table Name**: The name of the table (CSV filename).
- **Number of Rows**: The total number of rows in the table.
- **Number of Columns**: The total number of columns in the table.
- **Table Description**: A generated description of what the table contains.

For each **column** in your dataset:
- **Column Name**: The name of the column.
- **Sample Data**: A sample of 5 data points from the column.
- **Data Type**: The inferred data type (e.g., integer, float, string).
- **Column Description**: A description generated by the model for the column.

### Example of generated metadata:

| table_name | dataset_name | num_rows | num_columns | table_description         | column_name | sample_data       | data_type | column_description           |
|------------|--------------|----------|-------------|---------------------------|-------------|-------------------|-----------|------------------------------|
| patients   | ORCHID       | 1000     | 10          | Contains patient details.  | age        | [45, 60, 50, ...] | integer   | Age of the patient in years. |
| results    | ORCHID       | 500      | 6           | Stores test result data.   | result     | [positive, ...]   | string    | The result of the test.      |

## Data quality report

In addition to the data dictionary, the tool also generates a file called `data_quality.csv`. This file provides a summary of the data quality for each column, including the following information:

| column_name | num_missing_values | num_unique_values | data_type | data_quality_score | data_quality_description |
|-------------|--------------------|-------------------|-----------|--------------------|--------------------------|
| age         | 0                  | 10                | integer   | 1.0                | No missing values.       |
| result      | 5                  | 3                 | string    | 0.95               | Some missing values.     |

Where:
- **num_missing_values**: The number of missing values in the column.
- **num_unique_values**: The number of unique values in the column.
- **data_type**: The inferred data type of the column (e.g., integer, float, string).
- **data_quality_score**: A score (between 0 and 1) representing the quality of the column based on the completeness and diversity of data.
- **data_quality_description**: A text description of the data quality, such as the presence of missing values or the number of unique values.

This file can be used to assess the overall quality of your dataset, making it easier to identify columns that may require cleaning or further attention.

## Troubleshooting

### 1. If you encounter an error such as `model not found`, make sure you have set up Ollama correctly and the model is available.
   - Ensure that you can manually run the model using `ollama run deepseek-r1:1.5b` from the command line before using it in the Python script.

### 2. If the dependencies are not installing, make sure you're using the correct Python version and have all required libraries listed in `requirements.txt`.

### 3. If the dataset is very large, consider breaking it down into smaller CSV files for more efficient processing.

## Contributing

If you would like to contribute to this project, feel free to fork the repository and submit a pull request. Make sure to add tests and document any new features.

### To contribute:
1. Fork the repository.
2. Create a new branch (`git checkout -b feature-name`).
3. Make your changes and commit them (`git commit -am 'Add new feature'`).
4. Push to the branch (`git push origin feature-name`).
5. Open a pull request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Author

Raffaele Giancotti
